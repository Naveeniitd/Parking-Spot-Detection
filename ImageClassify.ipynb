{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57535c5-27a7-447f-9a77-0d52314383bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.1.3 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: scikit-image==0.19.3 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.19.3)\n",
      "Requirement already satisfied: numpy==1.23.4 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-learn==1.1.3->-r requirements.txt (line 1)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-learn==1.1.3->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-learn==1.1.3->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-image==0.19.3->-r requirements.txt (line 2)) (2.8.8)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-image==0.19.3->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-image==0.19.3->-r requirements.txt (line 2)) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-image==0.19.3->-r requirements.txt (line 2)) (2019.7.26.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-image==0.19.3->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from scikit-image==0.19.3->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: imagecodecs>=2019.11.28 in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (from tifffile>=2019.7.26->scikit-image==0.19.3->-r requirements.txt (line 2)) (2023.3.16)\n",
      "Requirement already satisfied: joblib in c:\\users\\navee\\.conda\\envs\\cvenv\\lib\\site-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf7c97a-e762-4111-98ed-5fcda3f3c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4360107-a661-45ff-abc2-c48f70fa0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data'\n",
    "categories = ['empty', 'not_empty']\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for category_idx, category in enumerate(categories):\n",
    "    for file in os.listdir(os.path.join(input_dir, category)):\n",
    "        img_path = os.path.join(input_dir, category, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (15, 15))\n",
    "        images.append(img.flatten())\n",
    "        labels.append(category_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10477ce-9b95-41a8-bd8d-6aeeb4525ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(images)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size =0.2, shuffle=True, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ad3117-98aa-4c21-9f99-2ba1d7f147a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'classifier': [SVC()], 'classifier__kernel': ['rbf', 'linear'], 'classifier__C': [1, 10, 100]},\n",
    "    {'classifier': [RandomForestClassifier()], 'classifier__n_estimators': [10, 50, 100], 'classifier__max_features': [1, 2, 3]},\n",
    "    {'classifier': [LogisticRegression()], 'classifier__C': [1, 10, 100]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cbe232-c460-442a-b477-fcad9bfe8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('classifier', SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcbea11-854c-41cb-ac17-785cdb3fa91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7240fae0-f136-4118-bff2-ccc0d663c2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;classifier&#x27;, SVC())]), n_jobs=-1,\n",
       "             param_grid=[{&#x27;classifier&#x27;: [SVC()], &#x27;classifier__C&#x27;: [1, 10, 100],\n",
       "                          &#x27;classifier__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;linear&#x27;]},\n",
       "                         {&#x27;classifier&#x27;: [RandomForestClassifier()],\n",
       "                          &#x27;classifier__max_features&#x27;: [1, 2, 3],\n",
       "                          &#x27;classifier__n_estimators&#x27;: [10, 50, 100]},\n",
       "                         {&#x27;classifier&#x27;: [LogisticRegression()],\n",
       "                          &#x27;classifier__C&#x27;: [1, 10, 100]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;classifier&#x27;, SVC())]), n_jobs=-1,\n",
       "             param_grid=[{&#x27;classifier&#x27;: [SVC()], &#x27;classifier__C&#x27;: [1, 10, 100],\n",
       "                          &#x27;classifier__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;linear&#x27;]},\n",
       "                         {&#x27;classifier&#x27;: [RandomForestClassifier()],\n",
       "                          &#x27;classifier__max_features&#x27;: [1, 2, 3],\n",
       "                          &#x27;classifier__n_estimators&#x27;: [10, 50, 100]},\n",
       "                         {&#x27;classifier&#x27;: [LogisticRegression()],\n",
       "                          &#x27;classifier__C&#x27;: [1, 10, 100]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;classifier&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('classifier', SVC())]), n_jobs=-1,\n",
       "             param_grid=[{'classifier': [SVC()], 'classifier__C': [1, 10, 100],\n",
       "                          'classifier__kernel': ['rbf', 'linear']},\n",
       "                         {'classifier': [RandomForestClassifier()],\n",
       "                          'classifier__max_features': [1, 2, 3],\n",
       "                          'classifier__n_estimators': [10, 50, 100]},\n",
       "                         {'classifier': [LogisticRegression()],\n",
       "                          'classifier__C': [1, 10, 100]}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fec022-8ee3-402d-9de5-ce8dc6159db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84508a87-cd14-41ae-9913-c0d605091b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_est.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a60a2d4a-ceae-43e2-bc05-4438e0b08290",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e8ac35-be00-4246-b2e8-c8679037c2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Park_classify']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_est, 'Park_classify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a053fa-1952-4479-8fed-31bbd1fca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_boxx(connected_components):\n",
    "    (totalLabels, label_ids, values, centroid) = connected_components\n",
    "\n",
    "    slots = []\n",
    "    coef = 1\n",
    "    for i in range(1, totalLabels):\n",
    "\n",
    "        # Now extract the coordinate points\n",
    "        x1 = int(values[i, cv2.CC_STAT_LEFT] * coef)\n",
    "        y1 = int(values[i, cv2.CC_STAT_TOP] * coef)\n",
    "        w = int(values[i, cv2.CC_STAT_WIDTH] * coef)\n",
    "        h = int(values[i, cv2.CC_STAT_HEIGHT] * coef)\n",
    "       \n",
    "        slots.append([x1, y1, w, h])\n",
    "\n",
    "    return slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be85010b-76f6-40b0-8049-bd3878a91401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "model = load('Park_classify')\n",
    "\n",
    "EMPTY = True\n",
    "NOT_EMPTY = False\n",
    "def classify_spot(img):\n",
    "    flat =[]\n",
    "    \n",
    "    img = cv2.resize(img, (15, 15))\n",
    "    flat.append(img.flatten())\n",
    "    flat = np.array(flat)\n",
    "    out = model.predict(flat)\n",
    "    \n",
    "\n",
    "    if out == 0:\n",
    "        return EMPTY\n",
    "    else:\n",
    "        return NOT_EMPTY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811aed4f-89f5-4835-a516-1212fed23dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_diff(img1, img2):\n",
    "    return np.abs(np.mean(img1) - np.mean(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739bd917-72e5-4af1-a040-e5540ed8d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path  = 'parking_1920_1080_loop.mp4'\n",
    "mask = 'mask_1920_1080.png'\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "mask = cv2.imread(mask, 0)\n",
    "connected_components = cv2.connectedComponentsWithStats(mask, 4, cv2.CV_32S)\n",
    "Boxs = bounding_boxx(connected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d713f6a6-6cba-41e1-8540-334188263ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "prev_frame = None\n",
    "steps = 30\n",
    "Boxs_status = [None for j in Boxs]\n",
    "diffs = [None for j in Boxs]\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read and display frame by frame\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read a frame\n",
    "    if frame_count % steps == 0 and prev_frame is not None :\n",
    "        for box_idx, box in enumerate(Boxs):\n",
    "            x1,y1,w,h = box\n",
    "            \n",
    "            spot_crop = frame[y1:y1+h, x1:x1+w, :]\n",
    "            diffs[box_idx] = pixel_diff(spot_crop, prev_frame[y1:y1+h, x1:x1+w, :])\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    if frame_count % steps == 0 :\n",
    "        if prev_frame is None:\n",
    "            arr_ = range(len(Boxs))\n",
    "        else:\n",
    "            arr_ = [j for j in np.argsort(diffs) if diffs[j] / np.amax(diffs) >0.4]\n",
    "        for box_idx in arr_:\n",
    "            box = Boxs[box_idx]\n",
    "            x1,y1,w,h = box\n",
    "            \n",
    "            spot_crop = frame[y1:y1+h, x1:x1+w, :]\n",
    "            status = classify_spot(spot_crop)\n",
    "            Boxs_status[box_idx] = status\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if frame_count % steps == 0:\n",
    "        prev_frame = frame.copy()\n",
    "\n",
    "    \n",
    "    for box_indx, box in enumerate(Boxs):\n",
    "        status = Boxs_status[box_indx]\n",
    "        x1,y1,w,h = Boxs[box_indx]\n",
    "\n",
    "        if status:\n",
    "            frame = cv2.rectangle(frame, (x1, y1), (x1+w, y1+h), (0, 255, 0), 2)\n",
    "        else:\n",
    "            frame = cv2.rectangle(frame, (x1, y1), (x1+w, y1+h), (0, 0, 255), 2)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    \n",
    "    cv2.putText(frame, 'Parking Spots Available: {} / {}'.format(str(sum(Boxs_status)), str(len(Boxs_status))), (100, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    cv2.imshow('Frame', frame)  # Display the frame\n",
    "\n",
    "    # Press 'q' on the keyboard to exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "    frame_count += 1\n",
    "# Release the VideoCapture object and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f490bc-98f9-4016-a600-0636fd06f3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060034e3-1773-4de8-80b1-2ec6cf8d8f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
